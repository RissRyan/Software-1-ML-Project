{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software project 1\n",
    "### by RISS Ryan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : First data anaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoeData = pd.read_csv('data/aoe_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224599\n",
      "Index(['Unnamed: 0', 'match_id', 'map', 'map_size', 'duration', 'dataset',\n",
      "       'difficulty', 'elo', 'p1_civ', 'p2_civ', 'p1_xpos', 'p2_xpos',\n",
      "       'p1_ypos', 'p2_ypos', 'winner'],\n",
      "      dtype='object')\n",
      "214003\n"
     ]
    }
   ],
   "source": [
    "print(aoeData.shape[0])\n",
    "print(aoeData.columns)\n",
    "aoeData = aoeData.dropna()\n",
    "print(aoeData.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only using map, duration, elo, p1_civ, p2_civ, p1_xpos, p2_xpos, p1_ypos, p2_ypos and winner.\n",
    "Roughy 10k entries have na values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the distribution of the picked features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eloData = aoeData['elo']\n",
    "\n",
    "minElo = eloData.min()\n",
    "maxElo = eloData.max()\n",
    "meanElo = eloData.mean()\n",
    "\n",
    "print(f\"Min elo: {minElo}\")\n",
    "print(f\"Max elo: {maxElo}\")\n",
    "print(f\"Mean elo: {meanElo}\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "ax.hist(eloData, bins=100)\n",
    "\n",
    "ax.set_title('Elo distribution')\n",
    "ax.set_xlabel('Elo')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "durationData = aoeData['duration']\n",
    "\n",
    "minDur = durationData.min()\n",
    "maxDur = durationData.max()\n",
    "meanDur = durationData.mean()\n",
    "\n",
    "print(f\"Min duration: {minDur}\")\n",
    "print(f\"Max duration: {maxDur}\")\n",
    "print(f\"Mean duration: {meanDur}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "ax.hist(durationData, bins=100)\n",
    "\n",
    "ax.set_title('Duration distribution')\n",
    "ax.set_xlabel('Duration (s)')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "civData = pd.concat([aoeData['p1_civ'], aoeData['p2_civ']])\n",
    "\n",
    "civsCount = civData.value_counts()\n",
    "\n",
    "civsCount = civsCount.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "civsCount.plot(kind='bar')\n",
    "\n",
    "plt.xlabel('Civilisation')\n",
    "plt.ylabel('Game count')\n",
    "plt.title('Games per civ')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapData = aoeData['map']\n",
    "\n",
    "mapsCount = mapData.value_counts()\n",
    "\n",
    "mapsCount = mapsCount.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "mapsCount.plot(kind='bar')\n",
    "\n",
    "plt.xlabel('Map')\n",
    "plt.ylabel('Game count')\n",
    "plt.title('Games per map')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(aoeData['p1_xpos'], aoeData['p1_ypos'], label='Player 1', alpha=0.2)\n",
    "\n",
    "plt.scatter(aoeData['p2_xpos'], aoeData['p2_ypos'], label='Player 2', alpha=0.2)\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Players\\' Positions')\n",
    "plt.legend() \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(aoeData, values='winner', index=['p1_civ', 'p2_civ'], aggfunc='mean')\n",
    "\n",
    "table = table.reset_index()\n",
    "\n",
    "table = table.rename(columns={'winner': 'mean_winner'})\n",
    "\n",
    "hmData = table.pivot('p1_civ', 'p2_civ', 'mean_winner')\n",
    "\n",
    "meanMeanWinner = table['mean_winner'].mean()\n",
    "\n",
    "print(f\"Mean mean winner: {meanMeanWinner}\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(hmData, annot=True, cmap='Blues', fmt=\".2f\", linewidths=.5, annot_kws={\"size\": 5, \"rotation\": 0})\n",
    "plt.title('Winner mean per match up')\n",
    "plt.xlabel('P2 Civ')\n",
    "plt.ylabel('P1 Civ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some map are overpresented (Arabia) and also some civ (Franks). Additionally some match ups are unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Using models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = aoeData[['duration', 'elo', 'p1_civ', 'p2_civ', 'p1_xpos', 'p2_xpos', 'p1_ypos', 'p2_ypos', 'winner']]\n",
    "\n",
    "target = features['winner']\n",
    "features = features.drop('winner', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization/Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresScaled = features.copy()\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "minMaxScaler = MinMaxScaler()\n",
    "\n",
    "# Duration standardized (outlier values)\n",
    "featuresScaled['duration'] = standardScaler.fit_transform(features[['duration']])\n",
    "\n",
    "# Min-Max scaling for the rest (gaussian or uniform distrib)\n",
    "columsToMinMax = ['elo', 'p1_xpos', 'p2_xpos', 'p1_ypos', 'p2_ypos']\n",
    "featuresScaled[columsToMinMax] = minMaxScaler.fit_transform(features[columsToMinMax])\n",
    "\n",
    "# One-hot encode civilizations\n",
    "featuresScaled = pd.get_dummies(featuresScaled, columns=['p1_civ', 'p2_civ'], prefix=['p1_civ', 'p2_civ'])\n",
    "\n",
    "# Séparation des données\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    featuresScaled,\n",
    "    target,\n",
    "    test_size=0.2,\n",
    "    random_state=1,\n",
    "    stratify=target\n",
    ")\n",
    "\n",
    "# Imprimer les valeurs de la première ligne après la normalisation\n",
    "print(X_train_scaled.iloc[0, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the mode\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=10, validation_split=0.0)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "conf_df = pd.DataFrame(conf_matrix, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Valeurs Réelles')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13697/13697 [==============================] - 27s 2ms/step - loss: 0.6914 - accuracy: 0.5248\n",
      "1071/1071 [==============================] - 2s 1ms/step\n",
      "13697/13697 [==============================] - 33s 2ms/step - loss: 0.6886 - accuracy: 0.5389\n",
      "1071/1071 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\GITHUB\\Software-1-ML-Project\\soft1.ipynb Cell 21\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GITHUB/Software-1-ML-Project/soft1.ipynb#X30sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m y_fold_train, y_fold_val \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39miloc[train_index], y_train\u001b[39m.\u001b[39miloc[val_index]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GITHUB/Software-1-ML-Project/soft1.ipynb#X30sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Train the model on the training data for this fold\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/GITHUB/Software-1-ML-Project/soft1.ipynb#X30sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_fold_train, y_fold_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GITHUB/Software-1-ML-Project/soft1.ipynb#X30sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Make predictions on the validation data for this fold\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GITHUB/Software-1-ML-Project/soft1.ipynb#X30sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m y_pred_proba \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_fold_val)\n",
      "File \u001b[1;32mc:\\Users\\ryan1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ryan1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1770\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1762\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1763\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n\u001b[0;32m   1764\u001b[0m     callbacks \u001b[39m=\u001b[39m callbacks_module\u001b[39m.\u001b[39mCallbackList(\n\u001b[0;32m   1765\u001b[0m         callbacks,\n\u001b[0;32m   1766\u001b[0m         add_history\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   1767\u001b[0m         add_progbar\u001b[39m=\u001b[39mverbose \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m   1768\u001b[0m         model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m   1769\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m-> 1770\u001b[0m         epochs\u001b[39m=\u001b[39mepochs,\n\u001b[0;32m   1771\u001b[0m         steps\u001b[39m=\u001b[39mdata_handler\u001b[39m.\u001b[39minferred_steps,\n\u001b[0;32m   1772\u001b[0m     )\n\u001b[0;32m   1774\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n",
      "File \u001b[1;32mc:\\Users\\ryan1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1341\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1341\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[0;32m   1342\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1343\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryan1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:496\u001b[0m, in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m iterator_ops\u001b[39m.\u001b[39mOwnedIterator:\n\u001b[0;32m    488\u001b[0m   \u001b[39m\"\"\"Creates an iterator for elements of this dataset.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \n\u001b[0;32m    490\u001b[0m \u001b[39m  The returned iterator implements the Python Iterator protocol.\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \n\u001b[0;32m    492\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[39m    An `tf.data.Iterator` for the elements of this dataset.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \n\u001b[0;32m    495\u001b[0m \u001b[39m  Raises:\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m \u001b[39m    RuntimeError: If not inside of tf.function and not executing eagerly.\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    499\u001b[0m     \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n",
      "File \u001b[1;32mc:\\Users\\ryan1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:705\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    702\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    703\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    704\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 705\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    706\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_iterator(dataset)\n\u001b[0;32m    708\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ryan1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:744\u001b[0m, in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    741\u001b[0m   \u001b[39m# fulltype is PRODUCT[ITERATOR[PRODUCT[...]]]\u001b[39;00m\n\u001b[0;32m    742\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[0;32m    743\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[1;32m--> 744\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m    745\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39mmake_iterator(ds_variant, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource)\n",
      "File \u001b[1;32mc:\\Users\\ryan1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3451\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3447\u001b[0m   _result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3448\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m-> 3451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_dataset\u001b[39m(input_dataset: Annotated[Any, _atypes\u001b[39m.\u001b[39mVariant], other_arguments, f, output_types, output_shapes, use_inter_op_parallelism:\u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, preserve_cardinality:\u001b[39mbool\u001b[39m\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, metadata:\u001b[39mstr\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Annotated[Any, _atypes\u001b[39m.\u001b[39mVariant]:\n\u001b[0;32m   3452\u001b[0m   \u001b[39mr\u001b[39m\u001b[39m\"\"\"Creates a dataset that applies `f` to the outputs of `input_dataset`.\u001b[39;00m\n\u001b[0;32m   3453\u001b[0m \n\u001b[0;32m   3454\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3466\u001b[0m \u001b[39m    A `Tensor` of type `variant`.\u001b[39;00m\n\u001b[0;32m   3467\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m   3468\u001b[0m   _ctx \u001b[39m=\u001b[39m _context\u001b[39m.\u001b[39m_context \u001b[39mor\u001b[39;00m _context\u001b[39m.\u001b[39mcontext()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Selecting features and target variable\n",
    "features = aoeData[['duration', 'elo', 'p1_civ', 'p2_civ', 'p1_xpos', 'p2_xpos', 'p1_ypos', 'p2_ypos', 'winner']]\n",
    "target = features['winner']\n",
    "features = features.drop('winner', axis=1)\n",
    "\n",
    "# Feature scaling and one-hot encoding\n",
    "featuresScaled = features.copy()\n",
    "standardScaler = StandardScaler()\n",
    "minMaxScaler = MinMaxScaler()\n",
    "\n",
    "featuresScaled['duration'] = standardScaler.fit_transform(features[['duration']])\n",
    "columnsToMinMax = ['elo', 'p1_xpos', 'p2_xpos', 'p1_ypos', 'p2_ypos']\n",
    "featuresScaled[columnsToMinMax] = minMaxScaler.fit_transform(features[columnsToMinMax])\n",
    "featuresScaled = pd.get_dummies(featuresScaled, columns=['p1_civ', 'p2_civ'], prefix=['p1_civ', 'p2_civ'])\n",
    "\n",
    "# Splitting the data\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "    featuresScaled,\n",
    "    target,\n",
    "    test_size=0.2,\n",
    "    random_state=1,\n",
    "    stratify=target\n",
    ")\n",
    "\n",
    "# Creating the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize lists to store accuracy scores\n",
    "accuracy_scores = []\n",
    "\n",
    "# Iterate over the folds\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    # Split the data into training and validation sets for this fold\n",
    "    X_fold_train, X_fold_val = X_train_scaled.iloc[train_index], X_train_scaled.iloc[val_index]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Train the model on the training data for this fold\n",
    "    model.fit(X_fold_train, y_fold_train, epochs=1, batch_size=10, validation_split=0.0)\n",
    "\n",
    "    # Make predictions on the validation data for this fold\n",
    "    y_pred_proba = model.predict(X_fold_val)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Calculate accuracy for this fold and store it\n",
    "    accuracy = accuracy_score(y_fold_val, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Print the accuracy scores for each fold\n",
    "print(\"Accuracy scores for each fold:\", accuracy_scores)\n",
    "\n",
    "# Calculate and print the mean accuracy across all folds\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "print(\"Mean accuracy:\", mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_fit_context' from 'sklearn.base' (c:\\Users\\ryan1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\GITHUB\\Software-1-ML-Project\\soft1.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GITHUB/Software-1-ML-Project/soft1.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GITHUB/Software-1-ML-Project/soft1.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/GITHUB/Software-1-ML-Project/soft1.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GITHUB/Software-1-ML-Project/soft1.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GITHUB/Software-1-ML-Project/soft1.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n",
      "File \u001b[1;32mc:\\Users\\ryan1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mThe :mod:`sklearn.ensemble` module includes ensemble-based methods for\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mclassification, regression and anomaly detection.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bagging\u001b[39;00m \u001b[39mimport\u001b[39;00m BaggingClassifier, BaggingRegressor\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseEnsemble\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_forest\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     ExtraTreesClassifier,\n\u001b[0;32m      9\u001b[0m     ExtraTreesRegressor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     RandomTreesEmbedding,\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ryan1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarnings\u001b[39;00m \u001b[39mimport\u001b[39;00m warn\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ClassifierMixin, RegressorMixin, _fit_context\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score, r2_score\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m DecisionTreeClassifier, DecisionTreeRegressor\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_fit_context' from 'sklearn.base' (c:\\Users\\ryan1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('data/aoe_data.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "# Utiliser 1/n % du jeu de données (ajustez n en fonction de vos besoins)\n",
    "subset_percentage = 3  # Utilisez 10% du jeu de données\n",
    "df_subset = df.sample(frac=(subset_percentage / 100), random_state=1)\n",
    "\n",
    "# Sélectionnez les colonnes pertinentes\n",
    "features = df_subset[['duration', 'elo', 'p1_civ', 'p2_civ', 'p1_xpos', 'p2_xpos', 'p1_ypos', 'p2_ypos', 'winner']]\n",
    "\n",
    "# One-hot encode the civilization columns for p1 and p2\n",
    "features = pd.get_dummies(features, columns=['p1_civ', 'p2_civ'], prefix=['p1_civ', 'p2_civ'])\n",
    "\n",
    "# Sélectionnez la variable cible (winner)\n",
    "target = features['winner']\n",
    "features = features.drop('winner', axis=1)\n",
    "\n",
    "# Divisez les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=1, stratify=target)\n",
    "\n",
    "# Normalisez les données (pour RandomForest, la normalisation n'est généralement pas nécessaire)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Créez le modèle Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "\n",
    "# Entraînez le modèle sur l'ensemble d'entraînement\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Faites des prédictions sur l'ensemble de test\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_rf = (y_pred_proba_rf > 0.5).astype(int)\n",
    "\n",
    "# Calculez la matrice de confusion\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Créez un DataFrame pour afficher la matrice de confusion\n",
    "conf_df_rf = pd.DataFrame(conf_matrix_rf, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "\n",
    "# Affichez la matrice de confusion\n",
    "print(conf_df_rf)\n",
    "\n",
    "# Visualisez la matrice de confusion avec seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title('Matrice de Confusion - Random Forest')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Valeurs Réelles')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
